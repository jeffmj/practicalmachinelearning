---
title: "Practical Machine Learning Class Project"
author: "Jeff Johnson"
date: "May 23, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/', warning=FALSE, message=FALSE, echo=TRUE)
set.seed(45678)
library("caret", lib.loc="~/R/win-library/3.3")
library("randomForest", lib.loc="~/R/win-library/3.3")
library("kernlab", lib.loc="~/R/win-library/3.3")

```
## Summary

Recent years have witnessed the growth of the fitness tracker industry, and at the same time, the growing use of the data generated by these devices to track the quantity of work done (steps taken, etc.). But quality of work is still an unexamined area. The data generated for this project was taken with the goal of checking the quality of an exercise (biceps curls in this case). Each end user was asked to perform the exercise correctly, and then to make 4 common mistakes. Our task is to take the data, and create a model that will determine which variant of the curl was performed, characterizing the data into 5 categories.There are 20 samples in the test data set, and I would like to get at least 19/20 correct. For a model to have a better than 80% chance of doing this, it needs an accuracy of better than 99%, as calculated using the binomial distribution. (pbinom(19, size=20, prob=0.99, lower.tail = FALSE)`. With the goal of achieving this level of accuracy, three models were built, a Gradient Boost Model, a State Vector Machine, and a Random Forest model. Each was chosen as they are good at classifying data.  

## Step 1: Getting and Cleaning the data
The data for this project is available here:
 
[training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) 

[testing data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

A description of the actual study is available [here](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises), and the resultant paper [here](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201).  

Given that we are told to use accelerometer data from the belt, forearm, arm and dumbell, cutting the data sets down to just these parameters (and the classe of the exercise) will greatly help reduce the compute time of the models. The following code downloads the data, filters on the words belt, arm, dumbell, and classe, and then removes columns with NA values. It then splits the training set into two, a training set, and a testing set, so that accuracy judgements can be made about the models without using the validation data. 

The validation data is trimmed the same way as the training set so that validation data can be used by the models.   
```{r getdata, cache=TRUE, echo=TRUE}
## set wd 
setwd("~/DataScience/PractMachineLearning")

##check to see if data file is already downloaded
if(!file.exists("pml-testing.csv")){      #if not there, go get it
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")  }
if(!file.exists("pml-training.csv")){      #if not there, go get it
   download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")  }
  
## Load file. 
validate <- read.csv("pml-testing.csv", na.strings=c("NA",""), header=TRUE)
fulltrain <- read.csv("pml-training.csv", na.strings=c("NA",""), header=TRUE)

# filter columns on: belt, forearm, arm, dumbell and classe
filter = grepl("belt|arm|dumbell|classe", names(fulltrain))
filtertrain = fulltrain[, filter]
filtervalid = validate[, filter]


##remove NA variables
cols.without.na = colSums(is.na(filtervalid)) == 0
trainnoNA = filtertrain[, cols.without.na]
validnoNA = filtervalid[, cols.without.na]


##Further sub split into training and cross validation sets
set.seed(489)
index_train <- createDataPartition(trainnoNA$classe, p=0.80, list=FALSE)

train <- trainnoNA[index_train,]
x <- train[,-40]
y <- train[,40]

xval <- trainnoNA[-index_train,]
xvalx <- xval[,-40]
xvaly <- xval[,40]
```
## Build a Gradient Boosting Machine Model to the data

The first attempt at developing an accurate model for this data will be a gradient boost machine, using the caret package.It then cross validates the model using the xval data created in the step above, building a confusion matrix of the results.  The code follows:

``` {r gbm,cache=TRUE}

fitControl <- trainControl(method = "cv",
                           number = 10)

gbm <- train(x,y,method = "gbm",trControl = fitControl, verbose = FALSE)


gbmcfm <- confusionMatrix(predict(gbm,xvalx),xvaly)
gbmcfm
```
As you can see, the accuracy is `r gbmcfm$overall["Accuracy"]`. Not bad, but not good enough. 

## Build a support vector machine 
Another model to try that is good at classification is a support vector machine, or SVM. The following code builds one from our data using caret, tests it with the cross validation data set, and creates a confusion matrix.

``` {r svm, cache = TRUE }
fitControl <- trainControl(method = "cv",
                           number = 10)

svm <- train(x,y, method = "svmRadial", trControl = fitControl)

svmcfm <- confusionMatrix(predict(svm,xvalx), xvaly)
svmcfm
```
This model is accurate to `r svmcfm$overall["Accuracy"]`. Again, not too bad, but worse than the GBM model. 

## Build a Random Forest Model

Using caret, a random forest model will be built with the training data, and checked for accuracy with the cross validation data created above from the split of the training set.  But without parallelization, this takes a really long time to run, so first set up parallelization, using techniques from this [article](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md).

``` {r rf}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)

fit <- train(x,y, method="rf",data = train, trControl = fitControl)

stopCluster(cluster)
registerDoSEQ()

fit

fitcm <- confusionMatrix(predict(fit,xvalx),xvaly)
fitcm
```
While this model takes far longer than the others to run, even with parallelization, it delivers an accuracy of `r fitcm$overall["Accuracy"]`, the best of the three models.  

## Compare the models

Now that three models have been generated, a comparison seems in order. The following table compares the accuracy of the three models:

``` {r table}
table <- rbind(gbmcfm$overall["Accuracy"],svmcfm$overall["Accuracy"],fitcm$overall["Accuracy"])
rownames(table)<- c("State Vector Machine","Gradient Boost Model","Random Forest Model")
table
```

The lowest error rate is from the Random Forest model, so use that to get the test set results. The code to generate the test set follows:

``` {r testset}
testset <- predict(fit,validnoNA[,-40])
testset
```


